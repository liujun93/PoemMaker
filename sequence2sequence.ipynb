{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from zhon import hanzi\n",
    "import collections\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "source_seq_len = 40\n",
    "target_seq_len = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('DATA/english.txt', 'r') as f:\n",
    "    source_text = f.read()\n",
    "with open('DATA/chinese.txt', 'r') as f:\n",
    "    target_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats:\n",
      "-----English Text As Source Text-----\n",
      "Num of Sentence: 100001\n",
      "Average num of words in a sentence: 32.89916100838992\n",
      "Max num of words in a sentence: 60\n",
      "\n",
      "-----Chinese Text As Target Text-----\n",
      "Num of Sentence: 100001\n",
      "Average num of words in a sentence: 24.73931260687393\n",
      "Max num of words in a sentence: 60\n"
     ]
    }
   ],
   "source": [
    "# data summary\n",
    "print('Dataset Stats:')\n",
    "print('-'*5 + 'English Text As Source Text' + '-'*5)\n",
    "sentences = source_text.split('\\n')\n",
    "word_count = [len(s.split()) for s in sentences]\n",
    "print('Num of Sentence: {}'.format(len(sentences)))\n",
    "print('Average num of words in a sentence: {}'.format(np.average(word_count)))\n",
    "print('Max num of words in a sentence: {}'.format(np.max(word_count)))\n",
    "print()\n",
    "print('-'*5 + 'Chinese Text As Target Text' + '-'*5)\n",
    "sentences = target_text.split('\\n')\n",
    "word_count = [len(s.split()) for s in sentences]\n",
    "print('Num of Sentence: {}'.format(len(sentences)))\n",
    "print('Average num of words in a sentence: {}'.format(np.average(word_count)))\n",
    "print('Max num of words in a sentence: {}'.format(np.max(word_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(text):\n",
    "    ## data clean punctuations\n",
    "    english_punct = string.punctuation\n",
    "    chinese_punct = hanzi.punctuation\n",
    "    punct = english_punct + chinese_punct\n",
    "    text = ''.join(char for char in text if char not in punct)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(text, vocab_size, is_target=False):\n",
    "    ## builiding vocabulary\n",
    "    count = [['<UNK>', -1], ['<PAD>', -2]] if not is_target else [['<UNK>', -1], ['<PAD>', -2], ['<GO>', -3], ['<EOS>', -4]]\n",
    "    counter = collections.Counter(text.split()).most_common(vocab_size-1)\n",
    "    count.extend(counter)\n",
    "    vocab = [word for word, _ in count]\n",
    "    ## building dictionaries\n",
    "    vocab_to_int = {w: i for i, w in enumerate(vocab)}\n",
    "    int_to_vocab = {i: w for i, w in enumerate(vocab)}\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int(text, map_dict, max_length=35, is_target=False):\n",
    "    # text to list of sentences\n",
    "    sentences = text.lower().split('\\n')\n",
    "    sentences = [s.split() for s in sentences]\n",
    "    \n",
    "    # text to int\n",
    "    text_to_idx = []\n",
    "    sent_to_idx = []\n",
    "    unk_idx = map_dict.get('<UNK>')\n",
    "    pad_idx = map_dict.get('<PAD>')\n",
    "    eos_idx = map_dict.get('<EOS>')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            sent_to_idx.append(map_dict.get(word, unk_idx))\n",
    "        if len(sent_to_idx) > max_length:\n",
    "            sent_to_idx = sent_to_idx[:max_length]\n",
    "        else:\n",
    "            sent_to_idx = sent_to_idx + [pad_idx] * (max_length - len(sent_to_idx))\n",
    "        if is_target:\n",
    "            sent_to_idx.append(eos_idx)\n",
    "        text_to_idx.append(sent_to_idx)\n",
    "        sent_to_idx = []\n",
    "    return text_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data clean\n",
    "source_text = data_clean(source_text)\n",
    "target_text = data_clean(target_text)\n",
    "\n",
    "# vocabularies\n",
    "source_vocab_to_int, source_int_to_vocab = build_dict(source_text, vocab_size)\n",
    "target_vocab_to_int, target_int_to_vocab = build_dict(target_text, vocab_size, is_target=True)\n",
    "\n",
    "# text_to_int\n",
    "source_text_to_int = text_to_int(source_text, source_vocab_to_int, source_seq_len)\n",
    "target_text_to_int = text_to_int(target_text, target_vocab_to_int, target_seq_len, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----English Example-----\n",
      "the general administration of customs alone has investigated and handled 13 officials at the level of provincial department head or equivalent and 50 officials at the level of provincial section head or equivalent who have violated discipline and law \n",
      "[2, 159, 262, 4, 1355, 1910, 15, 2807, 3, 1819, 921, 457, 24, 2, 169, 4, 624, 318, 1038, 69, 4213, 3, 722, 457, 24, 2, 169, 4, 624, 2448, 1038, 69, 4213, 103, 20, 2623, 785, 3, 102, 1]\n",
      "\n",
      "-----Chinese Example-----\n",
      "仅 中国 海关 就 查处 违纪 违法 的 厅局级 官员 十三 人  处 级 官员 五十 人 \n",
      "[1014, 7, 1655, 38, 1558, 2922, 1284, 3, 0, 538, 4630, 44, 785, 1126, 538, 1935, 44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "random_idx = 77\n",
    "print('-'*5 + 'English Example' + '-'*5)\n",
    "print(source_text.split('\\n')[random_idx])\n",
    "print(source_text_to_int[random_idx])\n",
    "print()\n",
    "print('-'*5 + 'Chinese Example' + '-'*5)\n",
    "print(target_text.split('\\n')[random_idx])\n",
    "print(target_text_to_int[random_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(source_text_to_int)\n",
    "Y = np.array(target_text_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer Hyperparameters\n",
    "encoder_hps = {'source_vocab_size': len(source_vocab_to_int),\n",
    "               'encoder_embed_size': 100,\n",
    "               'rnn_size': ,\n",
    "               'rnn_num_layers': ,\n",
    "               'source_seq_len': source_seq_len}\n",
    "\n",
    "# Decoder Layer Hyperparameters\n",
    "decoder_hps = {'target_vocab_size': len(target_vocab_to_int),\n",
    "               'encoder_embed_size': 100,\n",
    "               'rnn_size': ,\n",
    "               'rnn_num_layers': ,\n",
    "               'target_seq_len': target_seq_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
